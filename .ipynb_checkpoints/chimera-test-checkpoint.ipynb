{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from treelib import Tree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "class chimera():\n",
    "\n",
    "    def __init__(self, domain=None, obj=None):\n",
    "        \n",
    "        if type(obj)!=list:\n",
    "            obj = [obj]\n",
    "        \n",
    "        if type(obj[0])!=str:\n",
    "            raise ValueError(\"Please pass str only, not {}.\".format(type(obj[0])))\n",
    "        \n",
    "        if type(domain)!=str:\n",
    "            raise ValueError(\"Please pass str only, not {}.\".format(type(obj[0])))\n",
    "        \n",
    "        self.domain = domain\n",
    "        \n",
    "        self.data = []\n",
    "        for i in obj:\n",
    "            self.data.append(self.clean(i))\n",
    "        \n",
    "        self.cluster = []\n",
    "        self.keys = []\n",
    "        self.tree = Tree()\n",
    "\n",
    "        self.keybert = KeyBERT()\n",
    "        self.vectorizer = KeyphraseCountVectorizer()\n",
    "        self.sentbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    #clean text\n",
    "    def clean(self, raw_text):\n",
    "\n",
    "        text = re.sub(r\"https?://\\S+\", \"\", raw_text)\n",
    "        text = re.sub(r\"^[a-zA-Z0-9+_.-]+@[a-zA-Z0-9.-]+$\", \"\", text)\n",
    "        text = re.sub(r\"&amp;\", \" \", text)\n",
    "        text = re.sub(r\"<.*?>\", \" \", text)\n",
    "        text = re.sub(r\"^\\d$\", \" \", text)\n",
    "        text = text.replace(\"\\'\\'\",\"'\")\n",
    "        text = text.replace(\"\\\\\",\" \")\n",
    "\n",
    "        return text\n",
    "\n",
    "    #extract initial set of keywords from raw text\n",
    "    def extract_keyword(self, text=None, min_df=5, lang=\"english\", top_n=20):\n",
    "        \n",
    "        if text==None:\n",
    "            text = self.clean(self.data[0])\n",
    "        \n",
    "        results = set()\n",
    "    \n",
    "        keywords = self.keybert.extract_keywords(text, vectorizer=self.vectorizer, top_n=top_n, min_df= min_df, stop_words=lang)\n",
    "        for scored_keywords in keywords:\n",
    "            for keyword in scored_keywords:\n",
    "                if isinstance(keyword, str):\n",
    "                    results.add(keyword)\n",
    "                    \n",
    "        keywords = self.keybert.extract_keywords(text, keyphrase_ngram_range=(1, 3), top_n=top_n, min_df= min_df, stop_words=lang, use_maxsum=True,nr_candidates=top_n)\n",
    "        for scored_keywords in keywords:\n",
    "            for keyword in scored_keywords:\n",
    "                if isinstance(keyword, str):\n",
    "                    results.add(keyword)\n",
    "                    \n",
    "        return list(results)\n",
    "    \n",
    "    #extract communities and their respective names\n",
    "    def community_detecter(self, domain_keys, cluster_accuracy=75, min_cluster_size=2,batch_size=256):\n",
    "\n",
    "        df = pd.DataFrame(domain_keys).rename(columns={0:'Keyword'})\n",
    "        \n",
    "        cluster_name_list = []\n",
    "        corpus_sentences_list = []\n",
    "        df_all = []\n",
    "\n",
    "        corpus_set = set(df['Keyword'])\n",
    "        corpus_set_all = corpus_set\n",
    "        cluster = True\n",
    "\n",
    "        cluster_accuracy = cluster_accuracy / 100\n",
    "\n",
    "        while cluster:\n",
    "\n",
    "            corpus_sentences = list(corpus_set)\n",
    "            check_len = len(corpus_sentences)\n",
    "\n",
    "            corpus_embeddings = self.sentbert.encode(corpus_sentences, batch_size=batch_size, show_progress_bar=False, convert_to_tensor=True)\n",
    "            clusters = util.community_detection(corpus_embeddings, min_community_size=min_cluster_size, threshold=cluster_accuracy)\n",
    "\n",
    "            for keyword, cluster in enumerate(clusters):\n",
    "                for sentence_id in cluster[0:]:\n",
    "                    corpus_sentences_list.append(corpus_sentences[sentence_id])\n",
    "                    cluster_name_list.append(\"Cluster {}, #{} Elements \".format(keyword + 1, len(cluster)))\n",
    "\n",
    "            df_new = pd.DataFrame(None)\n",
    "            df_new['Cluster Name'] = cluster_name_list\n",
    "            df_new[\"Keyword\"] = corpus_sentences_list\n",
    "\n",
    "            df_all.append(df_new)\n",
    "            have = set(df_new[\"Keyword\"])\n",
    "\n",
    "            corpus_set = corpus_set_all - have\n",
    "            remaining = len(corpus_set)\n",
    "            if check_len==remaining:\n",
    "                break\n",
    "\n",
    "        df_new = pd.concat(df_all)\n",
    "        df = df.merge(df_new.drop_duplicates('Keyword'), how='left', on=\"Keyword\")\n",
    "\n",
    "        df['Length'] = df['Keyword'].astype(str).map(len)\n",
    "        df = df.sort_values(by=\"Length\", ascending=True)\n",
    "\n",
    "        df['Cluster Name'] = df.groupby('Cluster Name')['Keyword'].transform('first')\n",
    "        df.sort_values(['Cluster Name', \"Keyword\"], ascending=[True, True], inplace=True)\n",
    "\n",
    "        del df['Length']\n",
    "\n",
    "        col = df.pop(\"Keyword\")\n",
    "        df.insert(0, col.name, col)\n",
    "\n",
    "        col = df.pop('Cluster Name')\n",
    "        df.insert(0, col.name, col)\n",
    "\n",
    "        df.sort_values([\"Cluster Name\", \"Keyword\"], ascending=[True, True], inplace=True)\n",
    "\n",
    "        uncluster_percent = (remaining / len(domain_keys)) * 100\n",
    "        clustered_percent = 100 - uncluster_percent\n",
    "\n",
    "\n",
    "        return df[\"Cluster Name\"].to_list(), df[\"Keyword\"].to_list() \n",
    "    \n",
    "    #run to extract a two level knowledge tree\n",
    "    def pipeline(self, noise=False):\n",
    "        \n",
    "        self.cluster = []\n",
    "        self.keys = []\n",
    "        \n",
    "        domain_key = []\n",
    "\n",
    "        for i in range(len(self.data)):\n",
    "            self.keys.append(self.extract_keyword(self.data[i]))\n",
    "            domain_key += self.keys[i]\n",
    "            \n",
    "        domain_keys = list(set(domain_key))\n",
    "        \n",
    "        lv1, og = self.community_detecter(domain_keys)\n",
    "        \n",
    "        lv2, lv1_2 = self.community_detecter(lv1)\n",
    "        \n",
    "        for i in range(len(og)):\n",
    "            \n",
    "            l2 = lv2[lv1_2.index(lv1[i])]\n",
    "            if l2 is np.nan:\n",
    "                l2 = lv1[i]\n",
    "                if l2 is np.nan:\n",
    "                    l2 = og[i]\n",
    "                    l1 = np.nan\n",
    "                    l0 = np.nan\n",
    "                else: \n",
    "                    l1 = og[i]\n",
    "                    l0 = np.nan\n",
    "            else:\n",
    "                l1 = lv1[i]\n",
    "                l0 = og[i]\n",
    "            \n",
    "            item_data = {\n",
    "                \"2\":l2,\n",
    "                \"1\":l1,\n",
    "                \"0\":l0\n",
    "            }\n",
    "    \n",
    "            self.cluster.append(item_data)\n",
    "        \n",
    "        df = pd.DataFrame(self.cluster)\n",
    "        \n",
    "        if noise==False:\n",
    "            df = df.dropna(thresh=df.shape[1]-1, axis=0)\n",
    "        \n",
    "        return(df)\n",
    "    \n",
    "    def visualize(self, noise=False):\n",
    "        \n",
    "        self.tree = Tree()\n",
    "        \n",
    "        self.tree.create_node(self.domain,self.domain)\n",
    "        \n",
    "        for i in self.cluster:\n",
    "            for j in reversed(range(3)):\n",
    "                if i[str(j)] is not np.nan:\n",
    "                    try:\n",
    "                        if j==2 and i[str(j-1)] is not np.nan:\n",
    "                            if i[str(j)]!=self.domain:\n",
    "                                self.tree.create_node(i[str(j)],i[str(j)],parent=self.domain)\n",
    "                        else:\n",
    "                            if i[str(j)]!=i[str(j+1)] and len(i[str(j)].split(' '))>1:\n",
    "                                self.tree.create_node(i[str(j)],i[str(j)],parent=i[str(j+1)])\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        self.tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init mongo db for persistant data management\n",
    "import pymongo as pm\n",
    "\n",
    "mongo_url = \"mongodb://admin:password@mongodb:27017/\"\n",
    "client = pm.MongoClient(mongo_url)\n",
    "db = client[\"competencies\"][\"Big Data\"]\n",
    "\n",
    "text = []\n",
    "data = list(db.find({},{\"desc\":1}))\n",
    "\n",
    "for i in data:\n",
    "    text.append(i['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data\n",
      "├── advanced search\n",
      "│   ├── amazing search solutions\n",
      "│   ├── effective search\n",
      "│   ├── fast search solutions\n",
      "│   └── powerful search\n",
      "├── aggregation\n",
      "├── ai\n",
      "│   ├── ai agents\n",
      "│   └── artificial intelligence\n",
      "├── akka\n",
      "│   └── akka actors\n",
      "├── alteryx\n",
      "│   ├── alteryx course\n",
      "│   ├── alteryx designer\n",
      "│   └── alteryx developer\n",
      "├── analyses\n",
      "│   └── appropriate analysis\n",
      "├── analytics\n",
      "│   ├── analytics engine\n",
      "│   ├── analytics solutions\n",
      "│   └── data analytics\n",
      "├── apache kafka api\n",
      "│   ├── apache kafka stack\n",
      "│   └── custom apache kafka producers\n",
      "├── application\n",
      "│   ├── business applications\n",
      "│   ├── large applications\n",
      "│   └── various applications\n",
      "├── architecture\n",
      "│   └── common architectures\n",
      "├── availability\n",
      "│   └── high availability\n",
      "├── aws\n",
      "│   ├── aws platform\n",
      "│   └── cloud usage\n",
      "├── aws storage services\n",
      "│   └── various storage services\n",
      "├── azure\n",
      "│   ├── azure portal\n",
      "│   └── azure portal editor\n",
      "├── azure function\n",
      "│   ├── azure function apps\n",
      "│   ├── azure functions\n",
      "│   ├── azure functions app\n",
      "│   ├── azure functions bindings\n",
      "│   ├── microsoft azure functions\n",
      "│   └── own azure functions apps\n",
      "├── backup\n",
      "│   └── data backups\n",
      "├── basic methods\n",
      "│   ├── different methods\n",
      "│   └── many different methods\n",
      "├── basics\n",
      "│   └── very basics\n",
      "├── beginner\n",
      "│   ├── absolute beginner\n",
      "│   ├── absolute beginners\n",
      "│   └── beginner course\n",
      "├── bespoke data visualizations\n",
      "│   ├── different data visualization techniques\n",
      "│   └── enterprise data visualization tool\n",
      "├── big data\n",
      "│   ├── big data applications\n",
      "│   ├── big data architecture\n",
      "│   ├── big data engineering projects\n",
      "│   ├── big data technologies\n",
      "│   ├── big data use\n",
      "│   ├── complex big data applications\n",
      "│   ├── massive data\n",
      "│   │   └── big data demands\n",
      "│   └── right big data technology\n",
      "├── big data pipelines\n",
      "│   └── world big data pipelines\n",
      "├── business\n",
      "│   ├── large enterprises\n",
      "│   └── many enterprises\n",
      "├── charts\n",
      "│   ├── advanced charts\n",
      "│   └── basic charts\n",
      "├── classical computers\n",
      "│   └── current classical computing technology\n",
      "├── cloud\n",
      "│   ├── aws cloud\n",
      "│   ├── best cloud service\n",
      "│   ├── cloud computing\n",
      "│   └── cloud solutions\n",
      "├── cluster\n",
      "│   ├── cluster administration\n",
      "│   └── cluster types\n",
      "├── code\n",
      "│   └── source code\n",
      "├── components\n",
      "│   └── core components\n",
      "├── computer\n",
      "├── concept\n",
      "│   ├── basic concepts\n",
      "│   ├── important concepts\n",
      "│   └── most concepts\n",
      "├── confluent platform\n",
      "│   └── confluent platform functionalities\n",
      "├── consumers\n",
      "│   └── own consumers\n",
      "├── cv\n",
      "│   └── cv field\n",
      "├── dashboards\n",
      "│   ├── advanced dashboard techniques\n",
      "│   ├── analytic dashboards\n",
      "│   ├── basic dashboards\n",
      "│   └── interactive dashboards\n",
      "├── data\n",
      "│   └── data information\n",
      "├── data connection\n",
      "│   └── data connections\n",
      "├── data prep\n",
      "│   ├── basic data prep\n",
      "│   ├── data prep challenges\n",
      "│   ├── data prep process\n",
      "│   └── data preparation\n",
      "├── data science\n",
      "│   ├── data science jobs\n",
      "│   ├── data scientist\n",
      "│   ├── data scientists\n",
      "│   └── successful data scientist\n",
      "├── data science tools\n",
      "│   └── data science toolkit\n",
      "├── data structures\n",
      "│   ├── data structuring\n",
      "│   └── flexible data structures\n",
      "├── database\n",
      "│   └── backend database\n",
      "├── datasets\n",
      "│   ├── multiple datasets\n",
      "│   └── real datasets\n",
      "├── dax table calculations\n",
      "│   └── advanced dax calculations\n",
      "├── deep learning\n",
      "│   ├── deep learning methods\n",
      "│   └── deep learning models\n",
      "├── design web\n",
      "│   └── designing web\n",
      "├── development\n",
      "│   └── development environment\n",
      "├── devops\n",
      "│   └── devops environment\n",
      "├── dynamodb\n",
      "│   ├── amazon dynamodb\n",
      "│   ├── aws dynamodb\n",
      "│   ├── best practices dynamodb engineers use\n",
      "│   ├── dynamodb accelerator\n",
      "│   └── dynamodb tables\n",
      "├── elasticsearch\n",
      "│   ├── complete elasticsearch ecosystem\n",
      "│   ├── elasticsearch administration\n",
      "│   ├── elasticsearch architecture\n",
      "│   ├── elasticsearch book\n",
      "│   ├── elasticsearch cluster\n",
      "│   ├── elasticsearch clusters\n",
      "│   ├── elasticsearch cookbook\n",
      "│   ├── elasticsearch core concepts\n",
      "│   ├── elasticsearch nodes\n",
      "│   ├── elasticsearch optimization\n",
      "│   └── elasticsearch server functionalities\n",
      "├── elk\n",
      "│   └── elk stack\n",
      "├── experience\n",
      "│   └── industry experience\n",
      "├── features\n",
      "│   └── advanced features\n",
      "├── geospatial data\n",
      "│   └── geographical data\n",
      "├── git\n",
      "│   ├── advanced git concepts\n",
      "│   └── git places\n",
      "├── graph\n",
      "│   └── graphing data\n",
      "├── hadoop\n",
      "│   ├── apache hadoop\n",
      "│   ├── hadoop ecosystem\n",
      "│   └── hadoop professionals\n",
      "├── hdinsight\n",
      "│   └── hdinsight architecture\n",
      "├── ibm qiskit\n",
      "│   ├── ibm qiskit framework\n",
      "│   └── ibm qiskit technology\n",
      "├── implementations\n",
      "│   └── practical implementations\n",
      "├── improvement\n",
      "│   └── business improvement\n",
      "├── index\n",
      "│   ├── data indexing functionality\n",
      "│   └── indexing techniques\n",
      "├── industry\n",
      "│   └── software industry\n",
      "├── insight\n",
      "├── java\n",
      "│   ├── java application development\n",
      "│   └── java applications\n",
      "├── job interviews\n",
      "│   └── common job interview questions\n",
      "├── jobs\n",
      "├── kafka\n",
      "│   ├── apache kafka\n",
      "│   ├── efficient kafka programs\n",
      "│   ├── kafka components\n",
      "│   ├── kafka programming techniques\n",
      "│   ├── kafka programs\n",
      "│   └── pure kafka api\n",
      "├── kafka streams\n",
      "│   └── kafka streams library\n",
      "├── language\n",
      "│   └── human language\n",
      "├── logstash\n",
      "│   └── logstash data ingestion tool\n",
      "├── lucene\n",
      "│   └── apache lucene\n",
      "├── machine learning\n",
      "│   └── easy machine learning\n",
      "├── maps\n",
      "├── microservices\n",
      "│   └── reactive microservices\n",
      "├── natural language\n",
      "│   └── natural language processing\n",
      "├── new programming language\n",
      "│   └── python programming language\n",
      "├── nosql\n",
      "│   ├── data store\n",
      "│   ├── flexible nosql database service\n",
      "│   ├── many nosql databases\n",
      "│   ├── nosql data store\n",
      "│   ├── nosql technologies\n",
      "│   ├── nosql world\n",
      "│   └── ready nosql database\n",
      "├── open source search\n",
      "│   └── scalable open source search engine\n",
      "├── opportunities\n",
      "│   └── huge opportunities\n",
      "├── organization\n",
      "│   └── large organization\n",
      "├── patterns\n",
      "│   └── design patterns\n",
      "├── pipelines\n",
      "│   └── processing pipelines\n",
      "├── power bi\n",
      "│   ├── microsoft power bi\n",
      "│   ├── microsoft power bi tips\n",
      "│   ├── power bi desktop\n",
      "│   └── power bi service\n",
      "├── processing\n",
      "│   ├── efficient processing\n",
      "│   └── processing data\n",
      "├── products\n",
      "│   ├── new products\n",
      "│   └── retail products\n",
      "├── program\n",
      "│   ├── computer program\n",
      "│   ├── programming fundamentals\n",
      "│   ├── programming language\n",
      "│   └── programming languages\n",
      "├── projects\n",
      "│   ├── few projects\n",
      "│   ├── future projects\n",
      "│   ├── own projects\n",
      "│   ├── realistic projects\n",
      "│   └── small projects\n",
      "├── python\n",
      "│   └── python programming\n",
      "├── python spark\n",
      "│   ├── apache spark\n",
      "│   ├── apache spark architecture\n",
      "│   ├── spark architecture\n",
      "│   ├── spark execution\n",
      "│   ├── spark execution model\n",
      "│   ├── spark programming\n",
      "│   └── spark programming model\n",
      "├── quantum computing\n",
      "│   ├── other quantum computers\n",
      "│   ├── quantum algorithm\n",
      "│   ├── quantum computers\n",
      "│   ├── quantum computing concepts\n",
      "│   ├── quantum computing professionals\n",
      "│   └── real quantum computers\n",
      "├── queries\n",
      "│   ├── advanced queries\n",
      "│   └── complex queries\n",
      "├── recommender systems\n",
      "│   └── online recommender systems\n",
      "├── redis\n",
      "│   ├── redis cluster\n",
      "│   ├── redis environment\n",
      "│   └── redis sentinel\n",
      "├── redis data types\n",
      "│   └── redis data structure\n",
      "├── replication\n",
      "│   ├── replication issues\n",
      "│   └── replication tasks\n",
      "├── sas datasets\n",
      "│   ├── sas functions\n",
      "│   └── sas programming\n",
      "├── scalability\n",
      "│   └── scalable applications\n",
      "├── search\n",
      "│   ├── search relevance\n",
      "│   └── search results\n",
      "├── search tool\n",
      "│   └── search engine\n",
      "├── service\n",
      "├── skills\n",
      "│   ├── advanced skills\n",
      "│   └── practical skills\n",
      "├── statistics\n",
      "│   └── statistical research\n",
      "├── store data\n",
      "│   └── data storage needs\n",
      "├── streams\n",
      "│   ├── asynchronous data streams\n",
      "│   ├── data streams\n",
      "│   ├── different disparate data streams\n",
      "│   └── stream processing\n",
      "├── tableau\n",
      "│   ├── basic tableau concepts\n",
      "│   └── tableau journey\n",
      "├── technology\n",
      "├── time stream processing\n",
      "│   └── time stream processing applications\n",
      "├── tool\n",
      "│   ├── important tool\n",
      "│   └── popular tools\n",
      "├── topic\n",
      "│   ├── additional important topics\n",
      "│   ├── advanced topics\n",
      "│   ├── essential topics\n",
      "│   ├── important topic\n",
      "│   └── relevant topics\n",
      "├── triggers\n",
      "│   └── various triggers\n",
      "├── understanding\n",
      "│   ├── good understanding\n",
      "│   └── strong understanding\n",
      "├── unstructured data\n",
      "│   └── unstructured content\n",
      "├── variables\n",
      "│   └── new variables\n",
      "├── various formats\n",
      "│   └── various data formats\n",
      "├── visual studio\n",
      "│   └── visual studio code\n",
      "├── visualization\n",
      "│   ├── basic visualization methods\n",
      "│   ├── data visualization\n",
      "│   ├── important data visualization methods\n",
      "│   ├── insightful visualizations\n",
      "│   ├── many visualizations\n",
      "│   ├── rich visualizations\n",
      "│   ├── sophisticated visualizations\n",
      "│   └── useful visualizations\n",
      "└── volume\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = datetime.now()\n",
    "\n",
    "x = chimera(\"Big Data\",text)\n",
    "df = x.pipeline()\n",
    "x.visualize()\n",
    "\n",
    "t2 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:43.171458\n"
     ]
    }
   ],
   "source": [
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
